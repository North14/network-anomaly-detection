{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/North14/network-anomaly-detection/blob/main/notebooks/XGBOD_AnomalyDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9x-yYcsYjy8"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Setup\n",
        "# https://pyod.readthedocs.io/en/latest/\n",
        "# pyod for libraries in anomaly detection\n",
        "#\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "# Choose which dataset to use\n",
        "DATASET_MODE = \"kddcup99\"\n",
        "# options: \"RT-IOT2022\", \"TON-IoT-Fridge\"\n",
        "# \"cic-ids2018-fri02\", \"cic-ids2018-fri02-sub\", \"cic-ids2018-fri23\",\n",
        "# \"cic-ids2018-thu01\", \"cic-ids2018-thu15\", \"cic-ids2018-thu22\",\n",
        "# \"cic-ids2018-wed14\", \"cic-ids2018-wed21\", \"cic-ids2018-wed28\"\n",
        "DATASET_DIR = \"./datasets\"\n",
        "\n",
        "# Enable lightweight mode to avoid OOM\n",
        "# LIGHT_MODE = True          # set False for full data\n",
        "CHUNK_SIZE = 20000         # adjust for Colab (50kâ€“100k recommended)\n",
        "SAMPLE_LIMIT = 40000\n",
        "LIGHT_MODE = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQkmbpSHrHuB"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "#!pip3 install -U tensorflow\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report,\n",
        "    roc_curve, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
        ")\n",
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.models import Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "darD-NFmdCng"
      },
      "outputs": [],
      "source": [
        "\n",
        "if DATASET_MODE == \"RT-IOT2022\":\n",
        "    !pip3 install -U ucimlrepo\n",
        "    from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "    dataset = fetch_ucirepo(id=942)\n",
        "    X = dataset.data.features\n",
        "    y = dataset.data.targets\n",
        "    benign_labels = ['Thing_Speak', 'MQTT_Publish', 'NMAP_XMAS_TREE_SCAN', 'NMAP_TCP_scan',\n",
        "                 'NMAP_OS_DETECTION', 'NMAP_UDP_SCAN', 'Wipro_bulb', 'NMAP_FIN_SCAN',\n",
        "                 'DOS_SYN_Hping']\n",
        "    malicious_column = \"Attack_type\"\n",
        "    EXCLUDE_FROM_SCALING = []\n",
        "    #benign_labels = ['Thing_Speak', 'MQTT_Publish', 'Wipro_bulb',\n",
        "    #                 'NMAP_FIN_SCAN', 'NMAP_OS_DETECTION', 'NMAP_TCP_scan',\n",
        "    #                 'NMAP_UDP_SCAN', 'NMAP_XMAS_TREE_SCAN']\n",
        "\n",
        "\n",
        "elif DATASET_MODE.startswith(\"cic-ids2018-\"):\n",
        "    benign_labels = ['Benign']\n",
        "    malicious_column = \"Label\"\n",
        "    EXCLUDE_FROM_SCALING = ['time_sin', 'time_cos']\n",
        "    if DATASET_MODE == \"cic-ids2018-fri02\":\n",
        "      filename = \"/content/drive/MyDrive/datasets-anomaly-detection/CIC-IDS2018/Friday-02-03-2018_TrafficForML_CICFlowMeter.csv\"\n",
        "    elif DATASET_MODE == \"cic-ids2018-fri02-sub\":\n",
        "      filename = \"/content/drive/MyDrive/datasets-anomaly-detection/CIC-IDS2018/Friday-02-03-2018_TrafficForML_CICFlowMeter.subset.csv\"\n",
        "    elif DATASET_MODE == \"cic-ids2018-fri23\":\n",
        "      filename = \"/content/drive/MyDrive/datasets-anomaly-detection/CIC-IDS2018/Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\"\n",
        "    elif DATASET_MODE == \"cic-ids2018-thu01\":\n",
        "      filename = \"/content/drive/MyDrive/datasets-anomaly-detection/CIC-IDS2018/Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv\"\n",
        "    elif DATASET_MODE == \"cic-ids2018-thu15\":\n",
        "      filename = \"/content/drive/MyDrive/datasets-anomaly-detection/CIC-IDS2018/Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv\"\n",
        "    elif DATASET_MODE == \"cic-ids2018-thu22\":\n",
        "      filename = \"/content/drive/MyDrive/datasets-anomaly-detection/CIC-IDS2018/Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\"\n",
        "    elif DATASET_MODE == \"cic-ids2018-wed14\":\n",
        "      filename = \"/content/drive/MyDrive/datasets-anomaly-detection/CIC-IDS2018/Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv\"\n",
        "    elif DATASET_MODE == \"cic-ids2018-wed21\":\n",
        "      filename = \"/content/drive/MyDrive/datasets-anomaly-detection/CIC-IDS2018/Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv\"\n",
        "    elif DATASET_MODE == \"cic-ids2018-wed28\":\n",
        "      filename = \"/content/drive/MyDrive/datasets-anomaly-detection/CIC-IDS2018/Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv\"\n",
        "\n",
        "\n",
        "    df =  pd.read_csv(filename, low_memory=True)\n",
        "    # Attempt to parse timestamp automatically\n",
        "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
        "\n",
        "    # Extract hour, minute, and second\n",
        "    df[\"hour\"] = df[\"Timestamp\"].dt.hour.fillna(0)\n",
        "    df[\"minute\"] = df[\"Timestamp\"].dt.minute.fillna(0)\n",
        "    df[\"second\"] = df[\"Timestamp\"].dt.second.fillna(0)\n",
        "\n",
        "    # Compute cyclical time features\n",
        "    df[\"time_sin\"] = np.sin(2 * np.pi * ((df[\"hour\"] + (df[\"minute\"] + df[\"second\"] / 60) / 60) / 24))\n",
        "    df[\"time_cos\"] = np.cos(2 * np.pi * ((df[\"hour\"] + (df[\"minute\"] + df[\"second\"] / 60) / 60) / 24))\n",
        "\n",
        "    # Drop timestamp and raw time components (keep only sin/cos)\n",
        "    df = df.drop(columns=[\"Timestamp\", \"hour\", \"minute\", \"second\"])\n",
        "\n",
        "    X = df.drop(columns=[malicious_column])\n",
        "    y = df[[malicious_column]] if malicious_column in df.columns else None\n",
        "    print(f\"Loaded chunk shape: {df.shape}\")\n",
        "\n",
        "elif DATASET_MODE == \"TON-IoT-Fridge\":\n",
        "  benign_labels = False\n",
        "  malicious_column = \"label\"\n",
        "  EXCLUDE_FROM_SCALING = ['time_sin', 'time_cos']\n",
        "\n",
        "  filename = \"/content/drive/MyDrive/datasets-anomaly-detection/TON-IoT/Train_Test_IoT_Fridge.csv\"\n",
        "  df = pd.read_csv(filename, low_memory=True)\n",
        "  df[\"Timestamp\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
        "\n",
        "  # Extract hour, minute, and second\n",
        "  df[\"hour\"] = df[\"Timestamp\"].dt.hour.fillna(0)\n",
        "  df[\"minute\"] = df[\"Timestamp\"].dt.minute.fillna(0)\n",
        "  df[\"second\"] = df[\"Timestamp\"].dt.second.fillna(0)\n",
        "\n",
        "  # Compute cyclical time features\n",
        "  df[\"time_sin\"] = np.sin(2 * np.pi * ((df[\"hour\"] + (df[\"minute\"] + df[\"second\"] / 60) / 60) / 24))\n",
        "  df[\"time_cos\"] = np.cos(2 * np.pi * ((df[\"hour\"] + (df[\"minute\"] + df[\"second\"] / 60) / 60) / 24))\n",
        "\n",
        "  # Drop timestamp and raw time components (keep only sin/cos)\n",
        "  df = df.drop(columns=[\"Timestamp\", \"hour\", \"minute\", \"second\", \"time\", 'date', 'type'])\n",
        "\n",
        "  df['temp_condition'] = df['temp_condition'].astype(str).str.strip()\n",
        "\n",
        "  X = df.drop(columns=[malicious_column])\n",
        "  y = df[[malicious_column]] if malicious_column in df.columns else None\n",
        "  print(f\"Loaded chunk shape: {df.shape}\")\n",
        "\n",
        "elif DATASET_MODE == \"kddcup99\":\n",
        "  benign_labels = [b\"normal.\"]\n",
        "  malicious_column = \"labels\"\n",
        "  EXCLUDE_FROM_SCALING = []\n",
        "  from sklearn.datasets import fetch_kddcup99\n",
        "  df = fetch_kddcup99(subset=\"SA\", percent10=True, as_frame=True).frame\n",
        "  X = df.drop(columns=[malicious_column])\n",
        "  y = df[[malicious_column]] if malicious_column in df.columns else None\n",
        "  print(f\"Loaded chunk shape: {df.shape}\")\n",
        "\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "print(f\"Initial column names: {X.columns.values}\")\n",
        "print(X.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHLH6a-KdqGe"
      },
      "outputs": [],
      "source": [
        "\n",
        "# drop NaN or invalid\n",
        "X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "y = y.loc[X.index]\n",
        "\n",
        "\n",
        "\n",
        "num_cols = X.select_dtypes(include=[np.number]).columns\n",
        "cat_cols = X.select_dtypes(exclude=[np.number]).columns\n",
        "# Identify time features to exclude from scaling\n",
        "\n",
        "# Automatically exclude only those that exist\n",
        "excluded_features = [col for col in EXCLUDE_FROM_SCALING if col in num_cols]\n",
        "num_cols_to_scale = [col for col in num_cols if col not in excluded_features]\n",
        "\n",
        "if len(X) < 5e5:\n",
        "    log_transformer = FunctionTransformer(lambda arr: np.log1p(np.abs(arr)))\n",
        "else:\n",
        "    log_transformer = FunctionTransformer(lambda arr: arr)  # passthrough for big data\n",
        "\n",
        "numeric_transformer = Pipeline([\n",
        "    ('log', log_transformer),\n",
        "    ('scaler', RobustScaler(quantile_range=(5, 95)))\n",
        "])\n",
        "\n",
        "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "\n",
        "# Define preprocessor\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", numeric_transformer, num_cols_to_scale),\n",
        "    (\"passthrough\", \"passthrough\", excluded_features),\n",
        "    (\"cat\", categorical_transformer, cat_cols)\n",
        "])\n",
        "\n",
        "# Fit on a sample to reduce memory if data huge\n",
        "sample_rows = min(SAMPLE_LIMIT, len(X))\n",
        "preprocessor.fit(X.sample(sample_rows, random_state=42))\n",
        "\n",
        "if LIGHT_MODE:\n",
        "  # Transform in smaller batches\n",
        "  def transform_in_chunks(preprocessor, X, chunk_size=CHUNK_SIZE):\n",
        "      chunks = []\n",
        "      for i in range(0, len(X), chunk_size):\n",
        "          X_chunk = X.iloc[i:i+chunk_size]\n",
        "          X_trans = preprocessor.transform(X_chunk)\n",
        "          chunks.append(X_trans)\n",
        "      return np.vstack([c.toarray() if hasattr(c, \"toarray\") else c for c in chunks])\n",
        "  print(\"Transform\")\n",
        "  # Apply transformation safely\n",
        "  X_scaled = transform_in_chunks(preprocessor, X)\n",
        "else:\n",
        "# Apply transformations\n",
        "  X_scaled = preprocessor.transform(X)\n",
        "\n",
        "# print(np.min(X_scaled), np.max(X_scaled))\n",
        "# print(np.mean(X_scaled), np.std(X_scaled))\n",
        "\n",
        "\n",
        "# Get proper column names\n",
        "num_features = np.array(num_cols)\n",
        "cat_features = np.array([])\n",
        "if len(cat_cols) > 0:\n",
        "    cat_features = preprocessor.named_transformers_['cat'].get_feature_names_out(cat_cols)\n",
        "feature_names = np.concatenate([num_features, cat_features])\n",
        "\n",
        "\n",
        "# Construct final DataFrame\n",
        "X = pd.DataFrame(X_scaled, columns=feature_names, index=X.index)\n",
        "\n",
        "print(f\"Numeric cols: {len(num_cols)}, Categorical cols: {len(cat_cols)}\")\n",
        "print(f\"Final feature count: {X.shape[1]}\")\n",
        "print(X.head())\n",
        "\n",
        "print(cat_cols)\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VURRGUE-tiG3"
      },
      "outputs": [],
      "source": [
        "y_benign = y.loc[y[malicious_column].isin(benign_labels)]\n",
        "y_malicious = y.loc[~y[malicious_column].isin(benign_labels)]\n",
        "\n",
        "# Print ratio benign and malicious\n",
        "print(f\"Benign ratio: {len(y_benign) / len(y):.2f}\")\n",
        "print(f\"Malicious ratio: {len(y_malicious) / len(y):.2f}\")\n",
        "\n",
        "\n",
        "train_ratio = 0.65\n",
        "validation_ratio = 0.10\n",
        "test_ratio = 0.25\n",
        "# Split benign data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X.loc[y_benign.index], y_benign, test_size=test_ratio, random_state=4\n",
        ")\n",
        "# Split training data into training and validation sets\n",
        "# 75/25 of benign training data\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=validation_ratio/(train_ratio + validation_ratio), random_state=4\n",
        ")\n",
        "\n",
        "# Combine benign test data and malicious data for the overall test set\n",
        "X_test = pd.concat([X_test, X.loc[y_malicious.index]])\n",
        "y_test = pd.concat([y_test, y_malicious])\n",
        "\n",
        "# X_train, y_train -> benign training dataset -> (10630, 83), (10630, 1)\n",
        "# X_test_benign, y_test_benign -> benign testing dataset -> (1877, 83), (1877, 1)\n",
        "# X_test, y_test -> benign + malicious training dataset -> (112487, 83), (112487, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXum00p3OirX"
      },
      "outputs": [],
      "source": [
        "print(\"Benign label y_train\", y_train[malicious_column].unique())\n",
        "print(X_train.shape)\n",
        "print(\"Benign testing y_test_benign\", y_test[malicious_column].unique())\n",
        "print(X_test.shape)\n",
        "print(\"Benign validation y_val\", y_val[malicious_column].unique())\n",
        "print(X_val.shape)\n",
        "print(\"Malicious label y_test\", y_test[malicious_column].unique())\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XTosKmITp2L"
      },
      "outputs": [],
      "source": [
        "class AnomalyDetector(Model):\n",
        "  def __init__(self, input_dim, activation):\n",
        "    super(AnomalyDetector, self).__init__()\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      layers.Dense(48, activation=activation),\n",
        "      layers.Dense(16, activation=activation),\n",
        "      layers.Dense(8, activation=activation)])\n",
        "\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      layers.Dense(16, activation=activation),\n",
        "      layers.Dense(48, activation=activation),\n",
        "      layers.Dense(input_dim, activation=\"linear\")])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "\n",
        "autoencoder = AnomalyDetector(input_dim=X_train.shape[1], activation=\"relu\")\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "es = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
        "history = autoencoder.fit(X_train, X_train,\n",
        "          epochs=500,\n",
        "          batch_size=512,\n",
        "          validation_data=(X_val, X_val),\n",
        "          shuffle=True,\n",
        "          callbacks=[es])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEL3mnqieSk-"
      },
      "outputs": [],
      "source": [
        "#\n",
        "reconstructions = autoencoder.predict(X_test)\n",
        "loss = np.mean((X_test.to_numpy() - reconstructions)**2, axis=1)  # ensure numpy arrays\n",
        "\n",
        "#\n",
        "benign_mask = y_test[malicious_column].isin(benign_labels).values\n",
        "y_true = (~benign_mask).astype(int)  # 0 = benign, 1 = malicious\n",
        "\n",
        "#\n",
        "val_recon = autoencoder.predict(X_val)\n",
        "val_loss = np.mean(np.abs(X_val.to_numpy() - val_recon), axis=1)\n",
        "\n",
        "def best_threshold(y_true, loss):\n",
        "    thresholds = np.linspace(np.min(loss), np.max(loss), 2000)\n",
        "    f1_scores = [f1_score(y_true, (loss > t).astype(int)) for t in thresholds]\n",
        "    best_idx = np.argmax(f1_scores)\n",
        "    best_threshold = thresholds[best_idx]\n",
        "    print(f\"Best threshold: {best_threshold:.5f}, Best F1: {f1_scores[best_idx]:.4f}\")\n",
        "    return best_threshold\n",
        "\n",
        "def mad_threshold(val_loss, k):\n",
        "    med = np.median(val_loss)\n",
        "    mad = np.median(np.abs(val_loss - med))\n",
        "    return med + k * mad\n",
        "\n",
        "threshold = mad_threshold(val_loss, k=6.0)\n",
        "#\n",
        "# threshold = best_threshold(y_true, loss)\n",
        "#threshold = np.mean(val_loss) + 1 * np.std(val_loss)\n",
        "#print(f\"Threshold: {threshold:.5f}\")\n",
        "\n",
        "\n",
        "#\n",
        "y_pred = (loss > threshold).astype(int)\n",
        "\n",
        "#\n",
        "acc  = accuracy_score(y_true, y_pred)\n",
        "prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "rec  = recall_score(y_true, y_pred)\n",
        "f1   = f1_score(y_true, y_pred)\n",
        "auc  = roc_auc_score(y_true, loss)\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Benign\", \"Anomaly\"]))\n",
        "\n",
        "\n",
        "print(f\"\\nAccuracy:  {acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall:    {rec:.4f}\")\n",
        "print(f\"F1-score:  {f1:.4f}\")\n",
        "print(f\"AUC:       {auc:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Benign\", \"Anomaly\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "#\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(loss[benign_mask], bins=50, alpha=0.6, label='Benign', color=\"green\")\n",
        "plt.hist(loss[~benign_mask], bins=50, alpha=0.6, label='Malicious', color=\"red\")\n",
        "plt.axvline(x=threshold, color='black', linestyle='--', label='Threshold')\n",
        "plt.legend()\n",
        "plt.xlabel(\"Reconstruction Error (MSE)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Reconstruction Error Distribution\")\n",
        "plt.show()\n",
        "\n",
        "#\n",
        "fpr, tpr, _ = roc_curve(y_true, loss)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve â€“ Autoencoder Anomaly Detection\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}