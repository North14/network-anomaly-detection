{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1760977784700,
     "user": {
      "displayName": "Filip Andersson",
      "userId": "15415000852055350976"
     },
     "user_tz": -120
    },
    "id": "K9x-yYcsYjy8"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# Setup\n",
    "# https://pyod.readthedocs.io/en/latest/\n",
    "# pyod for libraries in anomaly detection\n",
    "#\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# Choose which dataset to use\n",
    "DATASET_MODE = \"cic-ids2018-wed14\"\n",
    "# options: \"RT-IOT2022\", \"TON-IoT-Fridge\"\n",
    "# \"cic-ids2018-fri02\", \"cic-ids2018-fri02-sub\", \"cic-ids2018-fri23\",\n",
    "# \"cic-ids2018-thu01\", \"cic-ids2018-thu15\", \"cic-ids2018-thu22\",\n",
    "# \"cic-ids2018-wed14\", \"cic-ids2018-wed21\", \"cic-ids2018-wed28\"\n",
    "\n",
    "# Enable lightweight mode to avoid OOM\n",
    "# LIGHT_MODE = True          # set False for full data\n",
    "CHUNK_SIZE = 20000         # adjust for Colab (50k–100k recommended)\n",
    "SAMPLE_LIMIT = 40000\n",
    "LIGHT_MODE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11347,
     "status": "ok",
     "timestamp": 1760977796066,
     "user": {
      "displayName": "Filip Andersson",
      "userId": "15415000852055350976"
     },
     "user_tz": -120
    },
    "id": "qQkmbpSHrHuB",
    "outputId": "c3858ede-558f-4541-fdeb-0ca44a6e578c"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip3 install -U tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report,\n",
    "    roc_curve, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42756,
     "status": "ok",
     "timestamp": 1760977838838,
     "user": {
      "displayName": "Filip Andersson",
      "userId": "15415000852055350976"
     },
     "user_tz": -120
    },
    "id": "darD-NFmdCng",
    "outputId": "ae1803de-314d-4ef0-feac-a832c32ee6c3"
   },
   "outputs": [],
   "source": [
    "\n",
    "if DATASET_MODE == \"RT-IOT2022\":\n",
    "    !pip3 install -U ucimlrepo\n",
    "    from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "    dataset = fetch_ucirepo(id=942)\n",
    "    X = dataset.data.features\n",
    "    y = dataset.data.targets\n",
    "    benign_labels = ['Thing_Speak', 'MQTT_Publish', 'NMAP_XMAS_TREE_SCAN', 'NMAP_TCP_scan',\n",
    "                 'NMAP_OS_DETECTION', 'NMAP_UDP_SCAN', 'Wipro_bulb', 'NMAP_FIN_SCAN',\n",
    "                 'DOS_SYN_Hping']\n",
    "    malicious_column = \"Attack_type\"\n",
    "    EXCLUDE_FROM_SCALING = []\n",
    "    #benign_labels = ['Thing_Speak', 'MQTT_Publish', 'Wipro_bulb',\n",
    "    #                 'NMAP_FIN_SCAN', 'NMAP_OS_DETECTION', 'NMAP_TCP_scan',\n",
    "    #                 'NMAP_UDP_SCAN', 'NMAP_XMAS_TREE_SCAN']\n",
    "\n",
    "\n",
    "elif DATASET_MODE.startswith(\"cic-ids2018-\"):\n",
    "    benign_labels = ['Benign']\n",
    "    malicious_column = \"Label\"\n",
    "    EXCLUDE_FROM_SCALING = ['time_sin', 'time_cos']\n",
    "    if DATASET_MODE == \"cic-ids2018-fri02\":\n",
    "      filename = \"/content/drive/MyDrive/datasets-anomaly-detection/CIC-IDS2018/Friday-02-03-2018_TrafficForML_CICFlowMeter.csv\"\n",
    "    elif DATASET_MODE == \"cic-ids2018-fri02-sub\":\n",
    "      filename = \"/content/drive/MyDrive/datasets-anomaly-detection/CIC-IDS2018/Friday-02-03-2018_TrafficForML_CICFlowMeter.subset.csv\"\n",
    "    elif DATASET_MODE == \"cic-ids2018-fri23\":\n",
    "      filename = \"/content/drive/MyDrive/datasets-anomaly-detection/CIC-IDS2018/Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\"\n",
    "    elif DATASET_MODE == \"cic-ids2018-thu01\":\n",
    "      filename = \"/content/drive/MyDrive/datasets-anomaly-detection/CIC-IDS2018/Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv\"\n",
    "    elif DATASET_MODE == \"cic-ids2018-thu15\":\n",
    "      filename = \"/content/drive/MyDrive/datasets-anomaly-detection/CIC-IDS2018/Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv\"\n",
    "    elif DATASET_MODE == \"cic-ids2018-thu22\":\n",
    "      filename = \"/content/drive/MyDrive/datasets-anomaly-detection/CIC-IDS2018/Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\"\n",
    "    elif DATASET_MODE == \"cic-ids2018-wed14\":\n",
    "      filename = \"/content/drive/MyDrive/datasets-anomaly-detection/CIC-IDS2018/Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv\"\n",
    "    elif DATASET_MODE == \"cic-ids2018-wed21\":\n",
    "      filename = \"/content/drive/MyDrive/datasets-anomaly-detection/CIC-IDS2018/Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv\"\n",
    "    elif DATASET_MODE == \"cic-ids2018-wed28\":\n",
    "      filename = \"/content/drive/MyDrive/datasets-anomaly-detection/CIC-IDS2018/Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv\"\n",
    "\n",
    "\n",
    "    df =  pd.read_csv(filename, low_memory=True)\n",
    "    # Attempt to parse timestamp automatically\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "\n",
    "    # Extract hour, minute, and second\n",
    "    df[\"hour\"] = df[\"Timestamp\"].dt.hour.fillna(0)\n",
    "    df[\"minute\"] = df[\"Timestamp\"].dt.minute.fillna(0)\n",
    "    df[\"second\"] = df[\"Timestamp\"].dt.second.fillna(0)\n",
    "\n",
    "    # Compute cyclical time features\n",
    "    df[\"time_sin\"] = np.sin(2 * np.pi * ((df[\"hour\"] + (df[\"minute\"] + df[\"second\"] / 60) / 60) / 24))\n",
    "    df[\"time_cos\"] = np.cos(2 * np.pi * ((df[\"hour\"] + (df[\"minute\"] + df[\"second\"] / 60) / 60) / 24))\n",
    "\n",
    "    # Drop timestamp and raw time components (keep only sin/cos)\n",
    "    df = df.drop(columns=[\"Timestamp\", \"hour\", \"minute\", \"second\"])\n",
    "\n",
    "    X = df.drop(columns=[malicious_column])\n",
    "    y = df[[malicious_column]] if malicious_column in df.columns else None\n",
    "    print(f\"Loaded chunk shape: {df.shape}\")\n",
    "\n",
    "elif DATASET_MODE == \"TON-IoT-Fridge\":\n",
    "  benign_labels = False\n",
    "  malicious_column = \"label\"\n",
    "  EXCLUDE_FROM_SCALING = ['time_sin', 'time_cos']\n",
    "\n",
    "  filename = \"/content/drive/MyDrive/datasets-anomaly-detection/TON-IoT/Train_Test_IoT_Fridge.csv\"\n",
    "  df = pd.read_csv(filename, low_memory=True)\n",
    "  df[\"Timestamp\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
    "\n",
    "  # Extract hour, minute, and second\n",
    "  df[\"hour\"] = df[\"Timestamp\"].dt.hour.fillna(0)\n",
    "  df[\"minute\"] = df[\"Timestamp\"].dt.minute.fillna(0)\n",
    "  df[\"second\"] = df[\"Timestamp\"].dt.second.fillna(0)\n",
    "\n",
    "  # Compute cyclical time features\n",
    "  df[\"time_sin\"] = np.sin(2 * np.pi * ((df[\"hour\"] + (df[\"minute\"] + df[\"second\"] / 60) / 60) / 24))\n",
    "  df[\"time_cos\"] = np.cos(2 * np.pi * ((df[\"hour\"] + (df[\"minute\"] + df[\"second\"] / 60) / 60) / 24))\n",
    "\n",
    "  # Drop timestamp and raw time components (keep only sin/cos)\n",
    "  df = df.drop(columns=[\"Timestamp\", \"hour\", \"minute\", \"second\", \"time\", 'date', 'type'])\n",
    "\n",
    "  df['temp_condition'] = df['temp_condition'].astype(str).str.strip()\n",
    "\n",
    "  X = df.drop(columns=[malicious_column])\n",
    "  y = df[[malicious_column]] if malicious_column in df.columns else None\n",
    "  print(f\"Loaded chunk shape: {df.shape}\")\n",
    "\n",
    "elif DATASET_MODE == \"KKDCup-10\":\n",
    "  benign_labels = \"normal.\"\n",
    "  malicious_column = \"label\"\n",
    "  EXCLUDE_FROM_SCALING = ['label', 'time_sin', 'time_cos']\n",
    "  filename = \"/content/drive/MyDrive/datasets-anomaly-detection/kddcup/kddcup.data_10_percent_corrected.csv\"\n",
    "  df = pd.read_csv(filename, low_memory=True)\n",
    "  X = df.drop(columns=[malicious_column])\n",
    "  y = df[[malicious_column]] if malicious_column in df.columns else None\n",
    "  print(f\"Loaded chunk shape: {df.shape}\")\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Initial column names: {X.columns.values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9869,
     "status": "ok",
     "timestamp": 1760977932194,
     "user": {
      "displayName": "Filip Andersson",
      "userId": "15415000852055350976"
     },
     "user_tz": -120
    },
    "id": "nHLH6a-KdqGe",
    "outputId": "d1b9d6fb-e1cc-4d92-8e1d-b3f915b49d72"
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: drop NaN or invalid\n",
    "X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "y = y.loc[X.index]\n",
    "\n",
    "\n",
    "\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "cat_cols = X.select_dtypes(exclude=[np.number]).columns\n",
    "# Identify time features to exclude from scaling\n",
    "\n",
    "# Automatically exclude only those that exist\n",
    "excluded_features = [col for col in EXCLUDE_FROM_SCALING if col in num_cols]\n",
    "num_cols_to_scale = [col for col in num_cols if col not in excluded_features]\n",
    "\n",
    "if len(X) < 5e5:\n",
    "    log_transformer = FunctionTransformer(lambda arr: np.log1p(np.abs(arr)))\n",
    "else:\n",
    "    log_transformer = FunctionTransformer(lambda arr: arr)  # passthrough for big data\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('log', log_transformer),\n",
    "    ('scaler', RobustScaler(quantile_range=(5, 95)))\n",
    "])\n",
    "\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "# Define preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, num_cols_to_scale),\n",
    "    (\"passthrough\", \"passthrough\", excluded_features),\n",
    "    (\"cat\", categorical_transformer, cat_cols)\n",
    "])\n",
    "\n",
    "# Fit on a sample to reduce memory if data huge\n",
    "sample_rows = min(SAMPLE_LIMIT, len(X))\n",
    "preprocessor.fit(X.sample(sample_rows, random_state=42))\n",
    "\n",
    "if LIGHT_MODE:\n",
    "  # Transform in smaller batches\n",
    "  def transform_in_chunks(preprocessor, X, chunk_size=CHUNK_SIZE):\n",
    "      chunks = []\n",
    "      for i in range(0, len(X), chunk_size):\n",
    "          X_chunk = X.iloc[i:i+chunk_size]\n",
    "          X_trans = preprocessor.transform(X_chunk)\n",
    "          chunks.append(X_trans)\n",
    "      return np.vstack([c.toarray() if hasattr(c, \"toarray\") else c for c in chunks])\n",
    "  print(\"Transform\")\n",
    "  # Apply transformation safely\n",
    "  X_scaled = transform_in_chunks(preprocessor, X)\n",
    "else:\n",
    "# Apply transformations\n",
    "  X_scaled = preprocessor.transform(X)\n",
    "\n",
    "# print(np.min(X_scaled), np.max(X_scaled))\n",
    "# print(np.mean(X_scaled), np.std(X_scaled))\n",
    "\n",
    "\n",
    "# Get proper column names\n",
    "num_features = np.array(num_cols)\n",
    "cat_features = np.array([])\n",
    "if len(cat_cols) > 0:\n",
    "    cat_features = preprocessor.named_transformers_['cat'].get_feature_names_out(cat_cols)\n",
    "feature_names = np.concatenate([num_features, cat_features])\n",
    "\n",
    "\n",
    "# Construct final DataFrame\n",
    "X = pd.DataFrame(X_scaled, columns=feature_names, index=X.index)\n",
    "\n",
    "print(f\"Numeric cols: {len(num_cols)}, Categorical cols: {len(cat_cols)}\")\n",
    "print(f\"Final feature count: {X.shape[1]}\")\n",
    "print(X.head())\n",
    "\n",
    "print(cat_cols)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "executionInfo": {
     "elapsed": 10941,
     "status": "error",
     "timestamp": 1760977849783,
     "user": {
      "displayName": "Filip Andersson",
      "userId": "15415000852055350976"
     },
     "user_tz": -120
    },
    "id": "81zXSfI9rycQ",
    "outputId": "5eeb5309-f463-4306-c1f6-2071d383e342"
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: drop NaN or invalid\n",
    "X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "y = y.loc[X.index]\n",
    "\n",
    "# TODO: preprocess X using one-hot encoder\n",
    "# cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "# encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "# X_encoded = encoder.fit_transform(X[cat_cols])\n",
    "# Create DataFrame for encoded features\n",
    "# encoded_df = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(cat_cols), index=X.index)\n",
    "# Combine encoded categorical features with numerical ones\n",
    "# num_df = X.drop(columns=cat_cols)\n",
    "# X_processed = pd.concat([num_df, encoded_df], axis=1)\n",
    "\n",
    "# TODO: normalize input features\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "cat_cols = X.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "#    ('log', FunctionTransformer(lambda x: np.log1p(np.abs(x)))),\n",
    "    ('log', FunctionTransformer(np.vectorize(lambda x: np.log1p(abs(x))) if len(X) < 5e5 else FunctionTransformer(lambda x: x))),  # skip log on big data\n",
    "    ('scaler', RobustScaler(quantile_range=(5, 95)))\n",
    "    # ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, num_cols),\n",
    "    (\"cat\", categorical_transformer, cat_cols)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Fit on a sample to reduce memory if data huge\n",
    "sample_rows = min(SAMPLE_LIMIT, len(X))\n",
    "X_sample = X.sample(sample_rows, random_state=42)\n",
    "preprocessor.fit(X_sample)\n",
    "\n",
    "if LIGHT_MODE:\n",
    "  # Transform in smaller batches\n",
    "  def transform_in_chunks(preprocessor, X, chunk_size=CHUNK_SIZE):\n",
    "      chunks = []\n",
    "      for i in range(0, len(X), chunk_size):\n",
    "          X_chunk = X.iloc[i:i+chunk_size]\n",
    "          X_trans = preprocessor.transform(X_chunk)\n",
    "          chunks.append(X_trans)\n",
    "      return np.vstack([c.toarray() if hasattr(c, \"toarray\") else c for c in chunks])\n",
    "  print(\"Transform\")\n",
    "  # Apply transformation safely\n",
    "  X_scaled = transform_in_chunks(preprocessor, X)\n",
    "else:\n",
    "# Apply transformations\n",
    "  X_scaled = preprocessor.fit_transform(X)\n",
    "\n",
    "# print(np.min(X_scaled), np.max(X_scaled))\n",
    "# print(np.mean(X_scaled), np.std(X_scaled))\n",
    "\n",
    "\n",
    "# Get proper column names\n",
    "num_features = num_cols\n",
    "cat_features = preprocessor.named_transformers_['cat'].get_feature_names_out(cat_cols)\n",
    "feature_names = np.concatenate([num_features, cat_features])\n",
    "\n",
    "# Construct final DataFrame\n",
    "X = pd.DataFrame(X_scaled, columns=feature_names, index=X.index)\n",
    "\n",
    "print(f\"Numeric cols: {len(num_cols)}, Categorical cols: {len(cat_cols)}\")\n",
    "print(f\"Final feature count: {X.shape[1]}\")\n",
    "print(X.head())\n",
    "\n",
    "print(cat_cols)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# metadata\n",
    "# print(rt_iot2022.metadata)\n",
    "# variable information\n",
    "# print(rt_iot2022.variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3202,
     "status": "ok",
     "timestamp": 1760977941740,
     "user": {
      "displayName": "Filip Andersson",
      "userId": "15415000852055350976"
     },
     "user_tz": -120
    },
    "id": "VURRGUE-tiG3",
    "outputId": "a6abb5d8-5e22-4a42-e52f-908035246556"
   },
   "outputs": [],
   "source": [
    "y_benign = y.loc[y[malicious_column].isin(benign_labels)]\n",
    "y_malicious = y.loc[~y[malicious_column].isin(benign_labels)]\n",
    "\n",
    "# Print ratio benign and malicious\n",
    "print(f\"Benign ratio: {len(y_benign) / len(y):.2f}\")\n",
    "print(f\"Malicious ratio: {len(y_malicious) / len(y):.2f}\")\n",
    "\n",
    "\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "# Split benign data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.loc[y_benign.index], y_benign, test_size=1 - train_ratio, random_state=4\n",
    ")\n",
    "# Split training data into training and validation sets\n",
    "# 75/25 of benign training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=test_ratio/(test_ratio + validation_ratio), random_state=4\n",
    ")\n",
    "\n",
    "# Combine benign test data and malicious data for the overall test set\n",
    "X_test = pd.concat([X_test, X.loc[y_malicious.index]])\n",
    "y_test = pd.concat([y_test, y_malicious])\n",
    "\n",
    "# X_train, y_train -> benign training dataset -> (10630, 83), (10630, 1)\n",
    "# X_test_benign, y_test_benign -> benign testing dataset -> (1877, 83), (1877, 1)\n",
    "# X_test, y_test -> benign + malicious training dataset -> (112487, 83), (112487, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1760977942658,
     "user": {
      "displayName": "Filip Andersson",
      "userId": "15415000852055350976"
     },
     "user_tz": -120
    },
    "id": "n89_gORZoO8J",
    "outputId": "c04cce2e-ceba-44e4-b35d-273470f465c6"
   },
   "outputs": [],
   "source": [
    "print(y.loc[y[malicious_column].isin(['ARP_poisioning', 'DOS_SYN_Hping'])].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84,
     "status": "ok",
     "timestamp": 1760977944626,
     "user": {
      "displayName": "Filip Andersson",
      "userId": "15415000852055350976"
     },
     "user_tz": -120
    },
    "id": "TXum00p3OirX",
    "outputId": "d9e35a4d-2e74-4947-ce2a-a777f46dc9b1"
   },
   "outputs": [],
   "source": [
    "print(\"Benign label y_train\", y_train[malicious_column].unique())\n",
    "print(X_train.shape)\n",
    "print(\"Benign testing y_test_benign\", y_test[malicious_column].unique())\n",
    "print(X_test.shape)\n",
    "print(\"Benign validation y_val\", y_val[malicious_column].unique())\n",
    "print(X_val.shape)\n",
    "print(\"Malicious label y_test\", y_test[malicious_column].unique())\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47218,
     "status": "ok",
     "timestamp": 1760977995532,
     "user": {
      "displayName": "Filip Andersson",
      "userId": "15415000852055350976"
     },
     "user_tz": -120
    },
    "id": "7XTosKmITp2L",
    "outputId": "719cf016-e8d9-475a-d10c-32bead4931fb"
   },
   "outputs": [],
   "source": [
    "class AnomalyDetector(Model):\n",
    "  def __init__(self, input_dim, activation):\n",
    "    super(AnomalyDetector, self).__init__()\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Dense(48, activation=activation),\n",
    "      layers.Dense(16, activation=activation),\n",
    "      layers.Dense(8, activation=activation)])\n",
    "\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(16, activation=activation),\n",
    "      layers.Dense(48, activation=activation),\n",
    "      layers.Dense(input_dim, activation=\"linear\")])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "\n",
    "\n",
    "autoencoder = AnomalyDetector(input_dim=X_train.shape[1], activation=\"relu\")\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "es = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "          epochs=500,\n",
    "          batch_size=512,\n",
    "          validation_data=(X_val, X_val),\n",
    "          shuffle=True,\n",
    "          callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4014,
     "status": "ok",
     "timestamp": 1760978117160,
     "user": {
      "displayName": "Filip Andersson",
      "userId": "15415000852055350976"
     },
     "user_tz": -120
    },
    "id": "TEL3mnqieSk-",
    "outputId": "015f6e0e-c006-4720-8002-65b823941acb"
   },
   "outputs": [],
   "source": [
    "#\n",
    "reconstructions = autoencoder.predict(X_test)\n",
    "loss = np.mean((X_test.to_numpy() - reconstructions)**2, axis=1)  # ensure numpy arrays\n",
    "\n",
    "#\n",
    "benign_mask = y_test[malicious_column].isin(benign_labels).values\n",
    "y_true = (~benign_mask).astype(int)  # 0 = benign, 1 = malicious\n",
    "\n",
    "#\n",
    "val_recon = autoencoder.predict(X_val)\n",
    "val_loss = np.mean(np.abs(X_val.to_numpy() - val_recon), axis=1)\n",
    "\n",
    "def best_threshold(y_true, loss):\n",
    "    thresholds = np.linspace(np.min(loss), np.max(loss), 2000)\n",
    "    f1_scores = [f1_score(y_true, (loss > t).astype(int)) for t in thresholds]\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    print(f\"Best threshold: {best_threshold:.5f}, Best F1: {f1_scores[best_idx]:.4f}\")\n",
    "    return best_threshold\n",
    "\n",
    "def mad_threshold(val_loss, k):\n",
    "    med = np.median(val_loss)\n",
    "    mad = np.median(np.abs(val_loss - med))\n",
    "    return med + k * mad\n",
    "\n",
    "threshold = mad_threshold(val_loss, k=6.0)\n",
    "#\n",
    "# threshold = best_threshold(y_true, loss)\n",
    "#threshold = np.mean(val_loss) + 1 * np.std(val_loss)\n",
    "#print(f\"Threshold: {threshold:.5f}\")\n",
    "\n",
    "\n",
    "#\n",
    "y_pred = (loss > threshold).astype(int)\n",
    "\n",
    "#\n",
    "acc  = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "rec  = recall_score(y_true, y_pred)\n",
    "f1   = f1_score(y_true, y_pred)\n",
    "auc  = roc_auc_score(y_true, loss)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Benign\", \"Anomaly\"]))\n",
    "\n",
    "\n",
    "print(f\"\\nAccuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")\n",
    "print(f\"AUC:       {auc:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Benign\", \"Anomaly\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(loss[benign_mask], bins=50, alpha=0.6, label='Benign', color=\"green\")\n",
    "plt.hist(loss[~benign_mask], bins=50, alpha=0.6, label='Malicious', color=\"red\")\n",
    "plt.axvline(x=threshold, color='black', linestyle='--', label='Threshold')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Reconstruction Error (MSE)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Reconstruction Error Distribution\")\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "fpr, tpr, _ = roc_curve(y_true, loss)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve – Autoencoder Anomaly Detection\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNOUsmz1FFSdr+zbm33PULm",
   "mount_file_id": "1sXCjGD4l1dKPoH3XfbWih_tmMzPiHISi",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
