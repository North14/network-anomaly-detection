{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1760977784700,
     "user": {
      "displayName": "Filip Andersson",
      "userId": "15415000852055350976"
     },
     "user_tz": -120
    },
    "id": "K9x-yYcsYjy8"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# Setup\n",
    "# https://pyod.readthedocs.io/en/latest/\n",
    "# pyod for libraries in anomaly detection\n",
    "#\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# Choose which dataset to use\n",
    "DATASET_MODE = \"bank\"\n",
    "# options: \"RT-IOT2022\", \"TON-IoT-Fridge\", \"kddcup99\"\n",
    "# \"cic-ids2018-fri02\", \"cic-ids2018-fri02-sub\", \"cic-ids2018-fri23\",\n",
    "# \"cic-ids2018-thu01\", \"cic-ids2018-thu15\", \"cic-ids2018-thu22\",\n",
    "# \"cic-ids2018-wed14\", \"cic-ids2018-wed21\", \"cic-ids2018-wed28\"\n",
    "DATASET_DIR = \"/home/artifact/Documents/network-anomaly-detection/datasets\"\n",
    "RESULTS_DIR = f\"/home/artifact/Documents/network-anomaly-detection/results/autoencoder/{DATASET_MODE}\"\n",
    "\n",
    "SAVING = False\n",
    "\n",
    "# Enable lightweight mode to avoid OOM\n",
    "# LIGHT_MODE = True          # set False for full data\n",
    "CHUNK_SIZE = 20000         # adjust for Colab (50kâ€“100k recommended)\n",
    "SAMPLE_LIMIT = 40000\n",
    "LIGHT_MODE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11347,
     "status": "ok",
     "timestamp": 1760977796066,
     "user": {
      "displayName": "Filip Andersson",
      "userId": "15415000852055350976"
     },
     "user_tz": -120
    },
    "id": "qQkmbpSHrHuB",
    "outputId": "c3858ede-558f-4541-fdeb-0ca44a6e578c"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "#!pip3 install -U tensorflow\n",
    "import os\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, average_precision_score, precision_recall_curve,\n",
    "    roc_curve, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42756,
     "status": "ok",
     "timestamp": 1760977838838,
     "user": {
      "displayName": "Filip Andersson",
      "userId": "15415000852055350976"
     },
     "user_tz": -120
    },
    "id": "darD-NFmdCng",
    "outputId": "ae1803de-314d-4ef0-feac-a832c32ee6c3"
   },
   "outputs": [],
   "source": [
    "\n",
    "if DATASET_MODE == \"RT-IOT2022\":\n",
    "    #!pip3 install -U ucimlrepo\n",
    "    from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "    dataset = fetch_ucirepo(id=942)\n",
    "    X = dataset.data.features\n",
    "    y = dataset.data.targets\n",
    "    benign_labels = ['Thing_Speak', 'MQTT_Publish', 'NMAP_XMAS_TREE_SCAN', 'NMAP_TCP_scan',\n",
    "                 'NMAP_OS_DETECTION', 'NMAP_UDP_SCAN', 'Wipro_bulb', 'NMAP_FIN_SCAN',\n",
    "                 'DOS_SYN_Hping']\n",
    "    malicious_column = \"Attack_type\"\n",
    "    EXCLUDE_FROM_SCALING = []\n",
    "    #benign_labels = ['Thing_Speak', 'MQTT_Publish', 'Wipro_bulb',\n",
    "    #                 'NMAP_FIN_SCAN', 'NMAP_OS_DETECTION', 'NMAP_TCP_scan',\n",
    "    #                 'NMAP_UDP_SCAN', 'NMAP_XMAS_TREE_SCAN']\n",
    "\n",
    "\n",
    "elif DATASET_MODE.startswith(\"cic-ids2018-\"):\n",
    "    benign_labels = ['Benign']\n",
    "    malicious_column = \"Label\"\n",
    "    EXCLUDE_FROM_SCALING = ['time_sin', 'time_cos']\n",
    "    if DATASET_MODE == \"cic-ids2018-fri02\":\n",
    "      filename = DATASET_DIR + \"/CIC-IDS2018/Friday-02-03-2018_TrafficForML_CICFlowMeter.csv\"\n",
    "    elif DATASET_MODE == \"cic-ids2018-fri02-sub\":\n",
    "      filename = DATASET_DIR + \"/CIC-IDS2018/Friday-02-03-2018_TrafficForML_CICFlowMeter.subset.csv\"\n",
    "    elif DATASET_MODE == \"cic-ids2018-fri23\":\n",
    "      filename = DATASET_DIR + \"/CIC-IDS2018/Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\"\n",
    "    elif DATASET_MODE == \"cic-ids2018-thu01\":\n",
    "      filename = DATASET_DIR + \"/CIC-IDS2018/Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv\"\n",
    "    elif DATASET_MODE == \"cic-ids2018-thu15\":\n",
    "      filename = DATASET_DIR + \"/CIC-IDS2018/Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv\"\n",
    "    elif DATASET_MODE == \"cic-ids2018-thu22\":\n",
    "      filename = DATASET_DIR + \"/CIC-IDS2018/Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\"\n",
    "    elif DATASET_MODE == \"cic-ids2018-wed14\":\n",
    "      filename = DATASET_DIR + \"/CIC-IDS2018/Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv\"\n",
    "    elif DATASET_MODE == \"cic-ids2018-wed21\":\n",
    "      filename = DATASET_DIR + \"/CIC-IDS2018/Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv\"\n",
    "    elif DATASET_MODE == \"cic-ids2018-wed28\":\n",
    "      filename = DATASET_DIR + \"/CIC-IDS2018/Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv\"\n",
    "\n",
    "\n",
    "    df =  pd.read_csv(filename, low_memory=True)\n",
    "    # Attempt to parse timestamp automatically\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "\n",
    "    # Extract hour, minute, and second\n",
    "    df[\"hour\"] = df[\"Timestamp\"].dt.hour.fillna(0)\n",
    "    df[\"minute\"] = df[\"Timestamp\"].dt.minute.fillna(0)\n",
    "    df[\"second\"] = df[\"Timestamp\"].dt.second.fillna(0)\n",
    "\n",
    "    # Compute cyclical time features\n",
    "    df[\"time_sin\"] = np.sin(2 * np.pi * ((df[\"hour\"] + (df[\"minute\"] + df[\"second\"] / 60) / 60) / 24))\n",
    "    df[\"time_cos\"] = np.cos(2 * np.pi * ((df[\"hour\"] + (df[\"minute\"] + df[\"second\"] / 60) / 60) / 24))\n",
    "\n",
    "    # Drop timestamp and raw time components (keep only sin/cos)\n",
    "    df = df.drop(columns=[\"Timestamp\", \"hour\", \"minute\", \"second\"])\n",
    "\n",
    "    X = df.drop(columns=[malicious_column])\n",
    "    y = df[[malicious_column]] if malicious_column in df.columns else None\n",
    "    print(f\"Loaded chunk shape: {df.shape}\")\n",
    "\n",
    "elif DATASET_MODE == \"TON-IoT-Fridge\":\n",
    "  benign_labels = False\n",
    "  malicious_column = \"label\"\n",
    "  EXCLUDE_FROM_SCALING = ['time_sin', 'time_cos']\n",
    "\n",
    "  filename = \"/content/drive/MyDrive/datasets-anomaly-detection/TON-IoT/Train_Test_IoT_Fridge.csv\"\n",
    "  df = pd.read_csv(filename, low_memory=True)\n",
    "  df[\"Timestamp\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
    "\n",
    "  # Extract hour, minute, and second\n",
    "  df[\"hour\"] = df[\"Timestamp\"].dt.hour.fillna(0)\n",
    "  df[\"minute\"] = df[\"Timestamp\"].dt.minute.fillna(0)\n",
    "  df[\"second\"] = df[\"Timestamp\"].dt.second.fillna(0)\n",
    "\n",
    "  # Compute cyclical time features\n",
    "  df[\"time_sin\"] = np.sin(2 * np.pi * ((df[\"hour\"] + (df[\"minute\"] + df[\"second\"] / 60) / 60) / 24))\n",
    "  df[\"time_cos\"] = np.cos(2 * np.pi * ((df[\"hour\"] + (df[\"minute\"] + df[\"second\"] / 60) / 60) / 24))\n",
    "\n",
    "  # Drop timestamp and raw time components (keep only sin/cos)\n",
    "  df = df.drop(columns=[\"Timestamp\", \"hour\", \"minute\", \"second\", \"time\", 'date', 'type'])\n",
    "\n",
    "  df['temp_condition'] = df['temp_condition'].astype(str).str.strip()\n",
    "\n",
    "  X = df.drop(columns=[malicious_column])\n",
    "  y = df[[malicious_column]] if malicious_column in df.columns else None\n",
    "  print(f\"Loaded chunk shape: {df.shape}\")\n",
    "\n",
    "elif DATASET_MODE == \"kddcup99\":\n",
    "  benign_labels = [b\"normal.\"]\n",
    "  malicious_column = \"labels\"\n",
    "  EXCLUDE_FROM_SCALING = []\n",
    "  from sklearn.datasets import fetch_kddcup99\n",
    "  df = fetch_kddcup99(subset=\"SA\", percent10=True, as_frame=True).frame\n",
    "  X = df.drop(columns=[malicious_column])\n",
    "  y = df[[malicious_column]] if malicious_column in df.columns else None\n",
    "  print(f\"Loaded chunk shape: {df.shape}\")\n",
    "\n",
    "elif DATASET_MODE == \"CelebA\":\n",
    "    # Using CelebA dataset as a placeholder for testing\n",
    "    benign_labels = ['0']  # Assuming '0' indicates benign in this context\n",
    "    malicious_column = \"class\"\n",
    "    EXCLUDE_FROM_SCALING = []\n",
    "    filename = DATASET_DIR + \"/DevNet/celeba_baldvsnonbald_normalised.csv\"\n",
    "    df = pd.read_csv(filename, low_memory=True)\n",
    "    X = df.drop(columns=[malicious_column])\n",
    "    y = df[[malicious_column]].astype(str) if malicious_column in df.columns else None\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    y = y.loc[X.index] if y is not None else None\n",
    "\n",
    "\n",
    "elif DATASET_MODE == \"bank\":\n",
    "    benign_labels = ['0']\n",
    "    malicious_column = \"class\"\n",
    "    EXCLUDE_FROM_SCALING = []\n",
    "    filename = DATASET_DIR + \"/DevNet/bank-additional-full_normalised.csv\"\n",
    "    df = pd.read_csv(filename, low_memory=True)\n",
    "    X = df.drop(columns=[malicious_column])\n",
    "    y = df[[malicious_column]].astype(str) if malicious_column in df.columns else None\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    y = y.loc[X.index] if y is not None else None\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Initial column names: {X.columns.values}\")\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9869,
     "status": "ok",
     "timestamp": 1760977932194,
     "user": {
      "displayName": "Filip Andersson",
      "userId": "15415000852055350976"
     },
     "user_tz": -120
    },
    "id": "nHLH6a-KdqGe",
    "outputId": "d1b9d6fb-e1cc-4d92-8e1d-b3f915b49d72"
   },
   "outputs": [],
   "source": [
    "\n",
    "# drop NaN or invalid\n",
    "X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "y = y.loc[X.index]\n",
    "\n",
    "\n",
    "\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "cat_cols = X.select_dtypes(exclude=[np.number]).columns\n",
    "# Identify time features to exclude from scaling\n",
    "\n",
    "# Automatically exclude only those that exist\n",
    "excluded_features = [col for col in EXCLUDE_FROM_SCALING if col in num_cols]\n",
    "num_cols_to_scale = [col for col in num_cols if col not in excluded_features]\n",
    "\n",
    "if len(X) < 5e5:\n",
    "    log_transformer = FunctionTransformer(lambda arr: np.log1p(np.abs(arr)))\n",
    "else:\n",
    "    log_transformer = FunctionTransformer(lambda arr: arr)  # passthrough for big data\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('log', log_transformer),\n",
    "    ('scaler', RobustScaler(quantile_range=(5, 95)))\n",
    "])\n",
    "\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "# Define preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, num_cols_to_scale),\n",
    "    (\"passthrough\", \"passthrough\", excluded_features),\n",
    "    (\"cat\", categorical_transformer, cat_cols)\n",
    "])\n",
    "\n",
    "# Fit on a sample to reduce memory if data huge\n",
    "sample_rows = min(SAMPLE_LIMIT, len(X))\n",
    "preprocessor.fit(X.sample(sample_rows, random_state=42))\n",
    "\n",
    "if LIGHT_MODE:\n",
    "  # Transform in smaller batches\n",
    "  def transform_in_chunks(preprocessor, X, chunk_size=CHUNK_SIZE):\n",
    "      chunks = []\n",
    "      for i in range(0, len(X), chunk_size):\n",
    "          X_chunk = X.iloc[i:i+chunk_size]\n",
    "          X_trans = preprocessor.transform(X_chunk)\n",
    "          chunks.append(X_trans)\n",
    "      return np.vstack([c.toarray() if hasattr(c, \"toarray\") else c for c in chunks])\n",
    "  print(\"Transform\")\n",
    "  # Apply transformation safely\n",
    "  X_scaled = transform_in_chunks(preprocessor, X)\n",
    "else:\n",
    "# Apply transformations\n",
    "  X_scaled = preprocessor.transform(X)\n",
    "\n",
    "# print(np.min(X_scaled), np.max(X_scaled))\n",
    "# print(np.mean(X_scaled), np.std(X_scaled))\n",
    "\n",
    "\n",
    "# Get proper column names\n",
    "num_features = np.array(num_cols)\n",
    "cat_features = np.array([])\n",
    "if len(cat_cols) > 0:\n",
    "    cat_features = preprocessor.named_transformers_['cat'].get_feature_names_out(cat_cols)\n",
    "feature_names = np.concatenate([num_features, cat_features])\n",
    "\n",
    "\n",
    "# Construct final DataFrame\n",
    "X = pd.DataFrame(X_scaled, columns=feature_names, index=X.index)\n",
    "\n",
    "print(f\"Numeric cols: {len(num_cols)}, Categorical cols: {len(cat_cols)}\")\n",
    "print(f\"Final feature count: {X.shape[1]}\")\n",
    "print(X.head())\n",
    "\n",
    "print(cat_cols)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47218,
     "status": "ok",
     "timestamp": 1760977995532,
     "user": {
      "displayName": "Filip Andersson",
      "userId": "15415000852055350976"
     },
     "user_tz": -120
    },
    "id": "7XTosKmITp2L",
    "outputId": "719cf016-e8d9-475a-d10c-32bead4931fb"
   },
   "outputs": [],
   "source": [
    "class AnomalyDetector(Model):\n",
    "  def __init__(self, input_dim, activation):\n",
    "    super(AnomalyDetector, self).__init__()\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Dense(48, activation=activation),\n",
    "      layers.Dense(16, activation=activation),\n",
    "      layers.Dense(8, activation=activation)])\n",
    "\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(16, activation=activation),\n",
    "      layers.Dense(48, activation=activation),\n",
    "      layers.Dense(input_dim, activation=\"linear\")])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "\n",
    "def mad_threshold(loss, k, scaled=True):\n",
    "    med = np.median(loss)\n",
    "    mad = np.median(np.abs(loss - med))\n",
    "    return med + k * mad\n",
    "\n",
    "autoencoder = AnomalyDetector(input_dim=X.shape[1], activation=\"relu\")\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "es = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_benign = X.loc[y[malicious_column].isin(benign_labels)]\n",
    "X_malicious = X.loc[~y[malicious_column].isin(benign_labels)]\n",
    "y_benign = y.loc[y[malicious_column].isin(benign_labels)]\n",
    "y_malicious = y.loc[~y[malicious_column].isin(benign_labels)]\n",
    "\n",
    "n_splits = 10\n",
    "benign_folds = list(KFold(n_splits=10, shuffle=True, random_state=4).split(X_benign))\n",
    "malicious_folds = list(KFold(n_splits=10, shuffle=True, random_state=4).split(X_malicious))\n",
    "\n",
    "# Performance for anomaly detection on k-fold\n",
    "# f1, auc, ap, negative recall (tnr), false omission rate, confusion matrix,  \n",
    "performance_metrics = {\n",
    "    \"model\": [],\n",
    "    \"dataset\": [],\n",
    "    \"kFold\": [],\n",
    "    \"kVal\": [],\n",
    "    \"f1_score\": [],\n",
    "    \"auc\": [],\n",
    "    \"ap\": [],\n",
    "    \"tp\": [],\n",
    "    \"fp\": [],\n",
    "    \"tn\": [],\n",
    "    \"fn\": [],\n",
    "    \"tnr\": [],\n",
    "    \"for\": [],\n",
    "    \"cm\": []\n",
    "}\n",
    "\n",
    "for fold, ((train_idx, test_idx), (mal_train_idx, mal_test_idx)) in enumerate(zip(benign_folds, malicious_folds)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "    \n",
    "    train_idx, test_idx = benign_folds[fold]\n",
    "    mal_train_idx, mal_test_idx = malicious_folds[fold]\n",
    "\n",
    "    # Use one k-fold for validation and remaining for training\n",
    "    # X_train_fold = X_benign.iloc[train_idx]\n",
    "    X_train_fold = X_benign.iloc[train_idx]\n",
    "    X_test_benign = X_benign.iloc[test_idx]\n",
    "    X_test_malicious = X_malicious.iloc[mal_test_idx]\n",
    "\n",
    "    # Create 15% validation split from training normals\n",
    "    X_train_fold, X_val_fold = train_test_split(\n",
    "        X_train_fold, test_size=0.15, random_state=4\n",
    "    )\n",
    "    y_val_fold = np.zeros(len(X_val_fold))\n",
    "    print(f\"Shape train: {X_train_fold.shape}, val: {X_val_fold.shape}, test benign: {X_test_benign.shape}, test malicious: {X_malicious.shape}\")\n",
    "    history = autoencoder.fit(\n",
    "        X_train_fold, X_train_fold,\n",
    "        epochs=500,\n",
    "        batch_size=512,\n",
    "        validation_data=(X_val_fold, X_val_fold),\n",
    "        shuffle=True,\n",
    "        verbose=0,\n",
    "        callbacks=[es]\n",
    "    )\n",
    "\n",
    "    # Test set = benign from fold + all malicious\n",
    "    X_test_fold = pd.concat([X_test_benign, X_test_malicious])\n",
    "    y_test_fold = np.concatenate([\n",
    "        np.zeros(len(X_test_benign)),\n",
    "        np.ones(len(X_test_malicious))\n",
    "    ])\n",
    "    print(f\"Test fold shape: {X_test_fold.shape}, labels shape: {y_test_fold.shape}\")\n",
    "    \n",
    "    # Predict reconstruction loss in MSE\n",
    "    recon = autoencoder.predict(X_test_fold)\n",
    "    loss = np.mean((X_test_fold.to_numpy() - recon) ** 2, axis=1)\n",
    "\n",
    "    # Reconstruction loss on validation set (Malignant) in MSE\n",
    "    val_recon = autoencoder.predict(X_val_fold)\n",
    "    val_loss = np.mean((X_val_fold.to_numpy() - val_recon) ** 2, axis=1)\n",
    "    # Calculate threshold using MAD\n",
    "    k_val = 6.0\n",
    "    threshold = mad_threshold(val_loss, k=k_val)\n",
    "    print(f\"Calculated MAD threshold (k={k_val}): {threshold:.4f}\")\n",
    "\n",
    "    y_pred = (loss > threshold).astype(int)\n",
    "    \n",
    "    # Evaluate\n",
    "    acc  = accuracy_score(y_test_fold, y_pred)\n",
    "    prec = precision_score(y_test_fold, y_pred, zero_division=0)\n",
    "    rec  = recall_score(y_test_fold, y_pred)\n",
    "    f1   = f1_score(y_test_fold, y_pred)\n",
    "    roc_auc  = roc_auc_score(y_test_fold, loss)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_fold, loss)\n",
    "\n",
    "    print(classification_report(y_test_fold, y_pred, target_names=[\"Benign\", \"Anomaly\"]))\n",
    "\n",
    "    cm = confusion_matrix(y_test_fold, y_pred)\n",
    "    #tp = cm[0,0]\n",
    "    #fn = cm[0,1]\n",
    "    #fp = cm[1,0]\n",
    "    #tn = cm[1,1]\n",
    "    tn = cm[0, 0]\n",
    "    fp = cm[0, 1]\n",
    "    fn = cm[1, 0]\n",
    "    tp = cm[1, 1]\n",
    "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0.0 # True Negative Rate (Specificity) Negative Recall NOTE: we want it high\n",
    "    for_rate = fn / (fn + tn) if (fn + tn) > 0 else 0.0 # False Omission Rate NOTE: we want it low\n",
    "    ap = average_precision_score(y_test_fold, loss)\n",
    "    precision, recall, _ = precision_recall_curve(y_test_fold, loss)\n",
    "    \n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(f\"AutoEncoder Evaluation - {DATASET_MODE} - Fold {fold+1}, k {k_val}\", fontsize=16)\n",
    "    fig.tight_layout(pad=4.0)\n",
    "    # Confusion Matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Benign\", \"Anomaly\"])\n",
    "    disp.plot(ax=axes[0, 0], cmap=plt.cm.Blues)\n",
    "    axes[0, 0].set_title(\"Confusion Matrix\")\n",
    "\n",
    "    # Reconstruction Error Distribution\n",
    "    axes[0, 1].hist(loss[y_test_fold == 0], bins=50, alpha=0.6, label='Benign', color=\"green\")\n",
    "    axes[0, 1].hist(loss[y_test_fold == 1], bins=50, alpha=0.6, label='Malicious', color=\"red\")\n",
    "    axes[0, 1].axvline(x=threshold, color='black', linestyle='--', label='Threshold')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].set_xlabel(\"Reconstruction Error with threshold marker (MSE)\")\n",
    "    axes[0, 1].set_ylabel(\"Frequency\")\n",
    "    axes[0, 1].set_xscale('log')\n",
    "    axes[0, 1].set_title(\"Reconstruction Error Distribution\")\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    axes[1, 0].plot(recall, precision, label=f'AP = {ap:.3f}')\n",
    "    axes[1, 0].set_xlabel('Recall')\n",
    "    axes[1, 0].set_ylabel('Precision')\n",
    "    axes[1, 0].set_title('Precision-Recall Curve')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    # ROC Curve\n",
    "    axes[1, 1].plot(fpr, tpr, label=f'AUC = {roc_auc:.3f}')\n",
    "    axes[1, 1].plot([0, 1], [0, 1], linestyle='--')  # diagonal baseline\n",
    "    axes[1, 1].set_xlabel('False Positive Rate')\n",
    "    axes[1, 1].set_ylabel('True Positive Rate')\n",
    "    axes[1, 1].set_title('ROC Curve')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    plt.show()\n",
    "    if SAVING:\n",
    "        fig.savefig(f\"{RESULTS_DIR}/fold_{fold+1}_evaluation.png\")\n",
    "\n",
    "    print(f\"Fold {fold} AUROC: {roc_auc:.4f}\")\n",
    "    performance_metrics[\"model\"].append(\"AutoEncoder\")\n",
    "    performance_metrics[\"dataset\"].append(DATASET_MODE)\n",
    "    performance_metrics[\"kFold\"].append(fold)\n",
    "    performance_metrics[\"kVal\"].append(k_val)\n",
    "    performance_metrics[\"f1_score\"].append(f1)\n",
    "    performance_metrics[\"auc\"].append(roc_auc)\n",
    "    \n",
    "    performance_metrics[\"ap\"].append(ap)\n",
    "\n",
    "    performance_metrics[\"tp\"].append(tp)\n",
    "    performance_metrics[\"fp\"].append(fp)\n",
    "    performance_metrics[\"tn\"].append(tn)\n",
    "    performance_metrics[\"fn\"].append(fn)\n",
    "    performance_metrics[\"tnr\"].append(tnr)\n",
    "    performance_metrics[\"for\"].append(for_rate)\n",
    "    performance_metrics[\"cm\"].append(cm)\n",
    "\n",
    "\n",
    "\n",
    "# Summarize overall performance\n",
    "aucs = performance_metrics[\"auc\"]\n",
    "aps = performance_metrics[\"ap\"]\n",
    "\n",
    "print(f\"\\nOverall AUROC: {np.mean(aucs):.4f} Â± {np.std(aucs):.4f}\")\n",
    "print(f\"Overall AUPR : {np.mean(aps):.4f} Â± {np.std(aps):.4f}\")\n",
    "\n",
    "# performance metrics -> pandas -> csv file\n",
    "if SAVING:\n",
    "    performance_df = pd.DataFrame(performance_metrics)\n",
    "    performance_df.to_csv(f\"{RESULTS_DIR}/performance_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNOUsmz1FFSdr+zbm33PULm",
   "mount_file_id": "1sXCjGD4l1dKPoH3XfbWih_tmMzPiHISi",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
